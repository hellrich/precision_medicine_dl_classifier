{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import * \n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from collections import Counter, namedtuple\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit\n",
    "\n",
    "\n",
    "## extra imports to set GPU options\n",
    "import tensorflow as tf\n",
    " \n",
    "# TensorFlow wizardry\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "session = tf.Session(config=config)\n",
    "tf.keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "maxlen = 300\n",
    "bonus_info_max=10000\n",
    "max_sentences = 10\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "vocab_size=40000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions for loading stuff\n",
    "from enum import Enum\n",
    "from os import listdir\n",
    "import json as j\n",
    "from collections import namedtuple\n",
    "\n",
    "#non-error prone training?\n",
    "def get_embedding_weights(input_dim, word_index, path=\"/home/hellrich/keras-test/embeddings/bio_nlp_win2_\", suffix=\"d.vec\",\n",
    "               embedding_dim=200):\n",
    "    def load_embedding_text(dim):\n",
    "        w2e = {}\n",
    "        for line in open(path+str(dim)+suffix):\n",
    "            line = line.strip().split(\" \")\n",
    "            if len(line) > 2:\n",
    "                w = line[0]\n",
    "                e = np.asarray(line[1:], dtype=\"float32\")\n",
    "                w2e[w] = e\n",
    "        return w2e\n",
    "    \n",
    "    index2word = {v : k for k, v in word_index.items()}\n",
    "    weights = np.zeros((input_dim, embedding_dim))\n",
    "    loaded = load_embedding_text(embedding_dim)\n",
    "    entries = 0\n",
    "    for i in range(1,input_dim):\n",
    "        w = index2word[i]\n",
    "        if w in loaded:\n",
    "            weights[i] = np.array(loaded[w])\n",
    "            entries += 1\n",
    "    return weights\n",
    "\n",
    "def make_embeddings(input_dim,input_length,weights,embedding_dim=200,trainable=False):\n",
    "    _weights = [np.copy(x) for x in weights]\n",
    "    return Embedding(input_dim, embedding_dim, weights=[weights],\n",
    "                      input_length=input_length, trainable=trainable) #input_dim + 1 ?\n",
    "\n",
    "BonusInfo = namedtuple('BonusInfo', [\"meshTags\",\"genes\", \"organisms\"])\n",
    "\n",
    "def parse_json(path=\"json-output\"):\n",
    "    pmid2info = {}\n",
    "    for json_file in [x for x in listdir(path) if x.endswith(\".json\")]:\n",
    "        with open(path+\"/\"+json_file, \"r\", encoding=\"utf8\") as open_json_file:\n",
    "            json = j.load(open_json_file)\n",
    "            if \"meshTags\" in json:\n",
    "                meshTags = \" \".join(json[\"meshTags\"])\n",
    "            else:\n",
    "                meshTags = \" \"\n",
    "            if \"genes\" in json:\n",
    "                genes = \" \".join(json[\"genes\"])\n",
    "            else:\n",
    "                genes = \" \"\n",
    "            if \"organisms\" in json:\n",
    "                organisms = \" \".join(json[\"organisms\"])\n",
    "            else:\n",
    "                organisms = \" \"\n",
    "            #abstract = json[\"abstract\"]\n",
    "            #title = json[\"title\"]\n",
    "            pmid = json[\"pubmedId\"]\n",
    "            pmid2info[pmid] = BonusInfo(meshTags, genes, organisms)\n",
    "    return pmid2info\n",
    "\n",
    "\n",
    "def read_data(data_file = \"20180622processedGoldStandardTopics.tsv\",  \n",
    "              maxlen=None, vocab_size=None, sentence_wise=None, bonus_info_max=False):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    pmids = []\n",
    "    with open(data_file) as data_file:\n",
    "        line_number = 0\n",
    "        for line in data_file:\n",
    "            if line_number > 0:\n",
    "                number, trec_topic_number, trec_doc_id, pm_rel_desc, disease_desc, \\\n",
    "                gene1_annotation_desc, gene1_name, gene2_annotation_desc, \\\n",
    "                gene2_name, gene3_annotation_desc, gene3_name, demographics_desc, \\\n",
    "                other_desc, relevance_score, title, abstract, major_mesh, \\\n",
    "                minor_mesh, trec_topic_disease, trec_topic_age, trec_topic_sex, \\\n",
    "                trec_topic_other1, trec_topic_other2, trec_topic_other3 = line.split(\"\\t\")\n",
    "                \n",
    "                text = title.replace(\"[\",\"\").replace(\"]\",\"\") + \" \" + abstract\n",
    "                text = text.lower().replace(\". \", \" . \").replace(\", \", \" , \").\\\n",
    "                        replace(\"? \", \" ? \").replace(\"! \", \" ! \")\n",
    "                texts.append(text)\n",
    "                \n",
    "                if pm_rel_desc == 'Human PM' or pm_rel_desc == 'Animal PM':\n",
    "                    labels.append(1.)\n",
    "                else:\n",
    "                    labels.append(0.)\n",
    "                pmids.append(trec_doc_id)    \n",
    "            line_number += 1\n",
    "                   \n",
    "    #fold_repeats:\n",
    "    text2indices = {}\n",
    "    for i, text in enumerate(texts):\n",
    "        if text in text2indices:\n",
    "            text2indices[text].append(i)\n",
    "        else:\n",
    "            text2indices[text] = [i]\n",
    "    for text, indices in text2indices.items():\n",
    "        if len(indices) > 1:\n",
    "            is_pm = False\n",
    "            for i in indices:\n",
    "                if labels[i] == 1.:\n",
    "                    is_pm = True\n",
    "            #remove all but first\n",
    "            for i in indices[1:]:\n",
    "                labels[i] = None\n",
    "                texts[i] = None\n",
    "                pmids[i] = None\n",
    "            if is_pm:\n",
    "                labels[indices[0]] = 1.\n",
    "    texts = [t for t in texts if t is not None]\n",
    "    labels = np.array([x for x in labels if x is not None])\n",
    "    pmids = [x for x in pmids if x is not None]     \n",
    "    \n",
    "    #see https://machinelearningmastery.com/prepare-text-data-deep-learning-keras/\n",
    "    if not vocab_size:\n",
    "        vocab_size = len({y for x in texts for y in x.split()})\n",
    "    t = Tokenizer(num_words=vocab_size)\n",
    "    t.fit_on_texts(texts)\n",
    "    \n",
    "    if sentence_wise:\n",
    "        max_sentences = sentence_wise[0]\n",
    "        max_sentence_length = sentence_wise[1]\n",
    "        list_of_list_of_sentences = [text.split(\" . \") for text in texts]\n",
    "        #trimming number of sentences\n",
    "        trimmed_list_of_list_of_sentences = []\n",
    "        for x in list_of_list_of_sentences:\n",
    "            if len(x) > max_sentences:\n",
    "                trimmed_list_of_list_of_sentences.append(x[:max_sentences])\n",
    "            else:\n",
    "                if len(x) < max_sentences:\n",
    "                    to_pad = max_sentences - len(x)\n",
    "                    for i in range(to_pad):\n",
    "                        x.append([\" \"])\n",
    "                trimmed_list_of_list_of_sentences.append(x)\n",
    "        list_of_sentence_matrices = [t.texts_to_sequences(x) for x in trimmed_list_of_list_of_sentences]\n",
    "        #trimming sentences for length\n",
    "        trimmed_list_of_sentence_matrices = [keras.preprocessing.sequence.pad_sequences(\n",
    "                            x, maxlen = max_sentence_length) for x in list_of_sentence_matrices]\n",
    "        texts = np.asarray(trimmed_list_of_sentence_matrices)\n",
    "    else:\n",
    "        texts = t.texts_to_sequences(texts)\n",
    "        if maxlen:\n",
    "            texts = keras.preprocessing.sequence.pad_sequences(texts, maxlen=maxlen)\n",
    "            \n",
    "    #bonus_info as bow\n",
    "    meshTags = []\n",
    "    genes = []\n",
    "    organisms = []\n",
    "    pmid2info = parse_json()\n",
    "    max_size = max(int(vocab_size/10), bonus_info_max)\n",
    "    for pmid in pmids:\n",
    "        info = pmid2info[pmid]\n",
    "        meshTags.append(info.meshTags)\n",
    "        genes.append(info.genes)\n",
    "        organisms.append(info.organisms)\n",
    "    num_meshTags = min(len({y for x in meshTags for y in x.split()}),max_size)\n",
    "    print(num_meshTags)\n",
    "    mesh_t = Tokenizer(num_words=num_meshTags)\n",
    "    mesh_t.fit_on_texts(meshTags)\n",
    "    meshTags = mesh_t.texts_to_matrix(meshTags, mode=\"binary\")\n",
    "    num_genes =  min(len({y for x in genes for y in x.split()}),max_size)\n",
    "    print(num_genes)\n",
    "    gene_t = Tokenizer(num_words=num_genes)\n",
    "    gene_t.fit_on_texts(genes)\n",
    "    genes = gene_t.texts_to_matrix(genes, mode=\"binary\")\n",
    "    num_orga =  min(len({y for x in organisms for y in x.split()}),max_size)\n",
    "    print(num_orga)\n",
    "    orga_t = Tokenizer(num_words=num_orga)\n",
    "    orga_t.fit_on_texts(organisms)\n",
    "    organisms = orga_t.texts_to_matrix(organisms, mode=\"binary\")\n",
    "    \n",
    "    return texts, labels, t.word_index, meshTags, genes, organisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "SmartTuner = namedtuple(\"SmartTuner\",[\"start_tuning_timestep\",\"timestep2mod\"])\n",
    "\n",
    "def experiment(model_provider, texts, labels, nfold=1, stratify=True, max_epochs=9, \n",
    "               optimizer=\"adam\", fine_tuner=None):\n",
    "    if stratify:\n",
    "        splitter = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=0)\n",
    "    else:\n",
    "        splitter = ShuffleSplit(n_splits=10, test_size=0.1, random_state=0)\n",
    "        \n",
    "    fold=0\n",
    "    accs=[]\n",
    "    for train_index, test_index in splitter.split(np.zeros(len(labels)), labels): #could be used for 10 fold\n",
    "        if fold >= nfold:\n",
    "            break\n",
    "        fold += 1\n",
    "        #get split\n",
    "        if not type(texts) == list:\n",
    "            texts = [texts]\n",
    "        train_x = []\n",
    "        test_x = []\n",
    "        for x in texts:\n",
    "            train_x.append(x[train_index])\n",
    "            test_x.append(x[test_index])\n",
    "        train_y = labels[train_index]\n",
    "        test_y = labels[test_index]\n",
    "         \n",
    "        model=model_provider()\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                    optimizer=optimizer,\n",
    "                    metrics=['acc'])\n",
    "        \n",
    "        for i in range(1,max_epochs+1):\n",
    "            if fine_tuner:\n",
    "                if i in fine_tuner.timestep2mod:\n",
    "                    lr = float(keras.backend.get_value(model.optimizer.lr)) * fine_tuner.timestep2mod[i]\n",
    "                    keras.backend.set_value(model.optimizer.lr, lr)\n",
    "                    if i == fine_tuner.start_tuning_timestep:\n",
    "                        model.layers[1].trainable = True\n",
    "                        model.compile(loss='binary_crossentropy',\n",
    "                        optimizer=optimizer,\n",
    "                        metrics=['acc'])  \n",
    "            model.fit(train_x, train_y, epochs=1, batch_size=50, verbose=0)\n",
    "            re = model.evaluate(train_x, train_y, verbose=0)\n",
    "            re_acc, re_loss = re[1],re[0]\n",
    "            te = model.evaluate(test_x, test_y, verbose=0)\n",
    "            test_acc, test_loss = te[1], te[0]\n",
    "\n",
    "            line = \"Epoch: \"+\"{:2}\".format(fold)+\"-\"+str(i)+\"\\tRe-Acc: \"+\"{:.3f}\".format(re_acc)+\"\\tTest-Acc: \"+\"{:.3f}\".format(test_acc)+\"\\tRe-Loss: \"+\"{:.3f}\".format(re_loss)+\"\\tTest-Loss: \"+\"{:.3f}\".format(test_loss)\n",
    "            print(line)\n",
    "        accs.append(test_acc) #no block scope has benefits...\n",
    "    acc = sum(accs) / nfold\n",
    "    print(\"X-Validation Accuracy:\",\"{:.3f}\".format(acc))\n",
    "    return acc\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple net (whole document as input)\n",
    "def simple_embedding_net(pre_trained=False):\n",
    "    embedding_in = Input(shape=(maxlen,), dtype='int32')\n",
    "    if not pre_trained:\n",
    "        embedding_layer = Embedding(input_dim=40000, output_dim=20, input_length=maxlen)(embedding_in)\n",
    "    else\n",
    "        raise Exception(\"Not implemented\")\n",
    "    embedding_layer = Flatten()(embedding_layer)\n",
    "    embedding_layer = Dropout(0.5)(embedding_layer)\n",
    "\n",
    "    inputs = [Input(shape=(meshTags.shape[1],)), \n",
    "              Input(shape=(genes.shape[1],)), Input(shape=(organisms.shape[1],))]\n",
    "\n",
    "    concatenated = Dropout(0.2)(keras.layers.concatenate([embedding_layer] + inputs))\n",
    "    output = Dense(1, activation='sigmoid')(concatenated)\n",
    "\n",
    "    model = keras.Model([embedding_in] + inputs, output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bonus info + sentence and document level lstms, see https://github.com/keras-team/keras/issues/5516#issuecomment-295016548\n",
    "def two_level_lstm(embedding_weights, vocab_size, maxlen, use_bonus_info=True):\n",
    "    def builder():\n",
    "        e = make_embeddings(input_dim=vocab_size, input_length=MAX_SEQUENCE_LENGTH, weights=embedding_weights)\n",
    "    \n",
    "        # Encode each timestep\n",
    "        bonus_info = [Input(shape=(meshTags.shape[1],)), \n",
    "                  Input(shape=(genes.shape[1],)), Input(shape=(organisms.shape[1],))] if use_bonus_info else None\n",
    "\n",
    "        in_sentence = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int64')\n",
    "        embedded_sentence = e(in_sentence)\n",
    "        lstm_sentence = LSTM(64)(embedded_sentence)\n",
    "        encoded_model = keras.Model(in_sentence, lstm_sentence)\n",
    "\n",
    "        sequence_input = Input(shape=(max_sentences, MAX_SEQUENCE_LENGTH), dtype='int64')\n",
    "        seq_encoded = TimeDistributed(encoded_model)(sequence_input)\n",
    "        seq_encoded = Dropout(0.2)(seq_encoded)\n",
    "\n",
    "        # Encode entire sentence\n",
    "        seq_encoded =  Dropout(0.2)(LSTM(64)(seq_encoded))\n",
    "\n",
    "        if bonus_info:\n",
    "            seq_encoded = Dropout(0.5)(keras.layers.concatenate([seq_encoded] + bonus_info))\n",
    "\n",
    "        # Prediction\n",
    "        out_layer = Dense(1, activation='sigmoid')(seq_encoded)\n",
    "        model = keras.Model([sequence_input] + bonus_info, out_layer)\n",
    "        return model\n",
    "    return builder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7537\n",
      "10000\n",
      "335\n"
     ]
    }
   ],
   "source": [
    "#loading stuff for cell below\n",
    "texts, labels, word_index, meshTags, genes, organisms = read_data(maxlen=maxlen, vocab_size=vocab_size, \n",
    "                     sentence_wise=(max_sentences,MAX_SEQUENCE_LENGTH), bonus_info_max=bonus_info_max)\n",
    "embedding_weights = get_embedding_weights(input_dim=vocab_size, word_index=word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1-1\tRe-Acc: 0.739\tTest-Acc: 0.718\tRe-Loss: 0.513\tTest-Loss: 0.528\n",
      "Epoch:  1-2\tRe-Acc: 0.774\tTest-Acc: 0.743\tRe-Loss: 0.471\tTest-Loss: 0.495\n",
      "Epoch:  1-3\tRe-Acc: 0.789\tTest-Acc: 0.754\tRe-Loss: 0.447\tTest-Loss: 0.475\n",
      "Epoch:  1-4\tRe-Acc: 0.797\tTest-Acc: 0.761\tRe-Loss: 0.426\tTest-Loss: 0.464\n",
      "Epoch:  1-5\tRe-Acc: 0.818\tTest-Acc: 0.758\tRe-Loss: 0.411\tTest-Loss: 0.459\n",
      "Epoch:  1-6\tRe-Acc: 0.832\tTest-Acc: 0.762\tRe-Loss: 0.379\tTest-Loss: 0.455\n",
      "Epoch:  1-7\tRe-Acc: 0.858\tTest-Acc: 0.767\tRe-Loss: 0.367\tTest-Loss: 0.448\n",
      "Epoch:  1-8\tRe-Acc: 0.859\tTest-Acc: 0.769\tRe-Loss: 0.339\tTest-Loss: 0.444\n",
      "Epoch:  2-1\tRe-Acc: 0.739\tTest-Acc: 0.711\tRe-Loss: 0.518\tTest-Loss: 0.538\n",
      "Epoch:  2-2\tRe-Acc: 0.769\tTest-Acc: 0.744\tRe-Loss: 0.464\tTest-Loss: 0.500\n",
      "Epoch:  2-3\tRe-Acc: 0.800\tTest-Acc: 0.765\tRe-Loss: 0.447\tTest-Loss: 0.485\n",
      "Epoch:  2-4\tRe-Acc: 0.800\tTest-Acc: 0.765\tRe-Loss: 0.427\tTest-Loss: 0.479\n",
      "Epoch:  2-5\tRe-Acc: 0.820\tTest-Acc: 0.751\tRe-Loss: 0.401\tTest-Loss: 0.481\n"
     ]
    }
   ],
   "source": [
    "#erreicht Epoch: 8\tRe-Acc: 0.875\tTest-Acc: 0.784\tRe-Loss: 0.300\tTest-Loss: 0.465\n",
    "#mit SmartTuner(4, {4: 0.5, 5: 4, 6: .7, 7: .7, 8: .7, 9: .7, 10: .7}) #higher boost&decay, gut ab 7i\n",
    "\n",
    "tuners =[SmartTuner(4, {4: 0.5, 5: 4, 6: .7, 7: .7, 8: .7, 9: .7, 10: .7})]\n",
    "model_provider = two_level_lstm(embedding_weights, vocab_size, maxlen, use_bonus_info=True)\n",
    "for tuner in tuners:\n",
    "    x = experiment(model_provider, [texts, meshTags, genes, organisms], labels, \n",
    "                   fine_tuner=tuner, max_epochs=8, nfold=10)\n",
    "    print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "SmartTuner = namedtuple(\"SmartTuner\",[\"start_tuning_timestep\",\"timestep2mod\"])\n",
    "\n",
    "tuners =[ SmartTuner(11,{}), #no finetuning\n",
    "        SmartTuner(3, {3: 0.5, 4: 3, 5: .8, 6: .8, 7: .8, 8: .8, 9: .8, 10: .8}), #original\n",
    "    #SmartTuner(3, {3: 0.5, 4: 4, 5: .7, 6: .7, 7: .7, 8: .7, 9: .7, 10: .7}) #higher boost&decay, gut ab 7i\n",
    "        ]\n",
    "\n",
    "embedding_weights = get_embedding_weights(input_dim=vocab_size, word_index=word_index)\n",
    "for tuner in tuners:\n",
    "    print(tuner)\n",
    "    e = make_embeddings(input_dim=vocab_size, input_length=MAX_SEQUENCE_LENGTH, weights=embedding_weights)  \n",
    "    bonus_info = [Input(shape=(meshTags.shape[1],)), \n",
    "              Input(shape=(genes.shape[1],)), Input(shape=(organisms.shape[1],))]\n",
    "\n",
    "    in_sentence = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int64')\n",
    "    embedded_sentence = e(in_sentence)\n",
    "    #conv_sentence = embedded_sentence\n",
    "    #conv_sentence =  Dropout(0.2)(embedded_sentence)\n",
    "    #conv_sentence =  Conv1D(256, 3, activation='relu')(conv_sentence)\n",
    "    #conv_sentence = GlobalMaxPooling1D()(conv_sentence)\n",
    "    #encoded_model = keras.Model(in_sentence, conv_sentence)\n",
    "\n",
    "\n",
    "    lstm_sentence = LSTM(64)(embedded_sentence)\n",
    "    encoded_model = keras.Model(in_sentence, lstm_sentence)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sentences, MAX_SEQUENCE_LENGTH), dtype='int64')\n",
    "    seq_encoded = TimeDistributed(encoded_model)(sequence_input)\n",
    "    seq_encoded = Dropout(0.2)(seq_encoded)\n",
    "\n",
    "    # Encode entire sentence\n",
    "    seq_encoded =  Dropout(0.2)(LSTM(64)(seq_encoded))\n",
    "    concat = Dropout(0.5)(keras.layers.concatenate([seq_encoded] + bonus_info))\n",
    "\n",
    "    # Prediction\n",
    "    out_layer = Dense(1, activation='sigmoid')(concat)\n",
    "    model = keras.Model([sequence_input] + bonus_info, out_layer)\n",
    "    r = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non-error prone training with bonus info as strings\n",
    "#loading stuff for cell below\n",
    "max_sentences = 10\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "vocab_size=40000\n",
    "\n",
    "#bionlp\n",
    "path = \"/home/hellrich/keras-test/embeddings/bio_nlp_win2_\"\n",
    "embedding_dim = 200\n",
    "suffix = \"d.vec\"\n",
    "\n",
    "texts, labels, word_index, meshTags, genes, organisms, dim_mesh, len_mesh, dim_genes, len_genes, dim_organisms, len_organisms = read_data(maxlen=maxlen, vocab_size=vocab_size, \n",
    "                                      return_word_index=True, sentence_wise=(max_sentences,MAX_SEQUENCE_LENGTH), \n",
    "                                                                  bonus_info_strings=True)\n",
    "\n",
    "def get_embedding_weights(input_dim, word_index, path=\"/home/hellrich/keras-test/embeddings/glove.6B.\", suffix=\"d.txt\",\n",
    "               embedding_dim=200):\n",
    "    def load_embedding_text(dim):\n",
    "        w2e = {}\n",
    "        for line in open(path+str(dim)+suffix):\n",
    "            line = line.strip().split(\" \")\n",
    "            if len(line) > 2:\n",
    "                w = line[0]\n",
    "                e = np.asarray(line[1:], dtype=\"float32\")\n",
    "                w2e[w] = e\n",
    "        return w2e\n",
    "    \n",
    "    index2word = {v : k for k, v in word_index.items()}\n",
    "    weights = np.zeros((input_dim, embedding_dim))\n",
    "    loaded = load_embedding_text(embedding_dim)\n",
    "    entries = 0\n",
    "    for i in range(1,input_dim):\n",
    "        w = index2word[i]\n",
    "        if w in loaded:\n",
    "            weights[i] = np.array(loaded[w])\n",
    "            entries += 1\n",
    "    return weights\n",
    "\n",
    "def make_embeddings(input_dim,input_length,weights,embedding_dim=200,trainable=False):\n",
    "    _weights = [np.copy(x) for x in weights]\n",
    "    return Embedding(input_dim, embedding_dim, weights=[weights],\n",
    "                      input_length=input_length, trainable=trainable) #input_dim + 1 ?\n",
    "\n",
    "\n",
    "\n",
    "from collections import namedtuple\n",
    "SmartTuner = namedtuple(\"SmartTuner\",[\"start_tuning_timestep\",\"timestep2mod\"])\n",
    "\n",
    "tuners =[ #SmartTuner(11,{}), #no finetuning\n",
    "        #SmartTuner(3, {3: 0.5, 4: 3, 5: .8, 6: .8, 7: .8, 8: .8, 9: .8, 10: .8}), #original\n",
    "    SmartTuner(3, {3: 0.5, 4: 4, 5: .7, 6: .7, 7: .7, 8: .7, 9: .7, 10: .7}), #higher boost&decay, gut ab 7i\n",
    "    SmartTuner(5, {5: 0.5, 6: 4, 7: .7, 8: .7, 9: .7, 10: .7}) #late and aggressive\n",
    "        ]\n",
    "\n",
    "\n",
    "for tuner in tuners:\n",
    "    print(tuner)\n",
    "    e = make_embeddings(input_dim=vocab_size, input_length=maxlen,weights=embedding_weights) \n",
    "\n",
    "    bonus_info = [\n",
    "       Input(shape=(len_mesh,)),\n",
    "       Input(shape=(len_genes,)),\n",
    "       Input(shape=(len_organisms,))\n",
    "    ]\n",
    "    \n",
    "    embedded_bonus_info = [\n",
    "        Flatten()(Embedding(dim_mesh, 20, input_length=len_mesh)(bonus_info[0])),\n",
    "        Flatten()(Embedding(dim_genes, 20, input_length=len_genes)(bonus_info[1])),\n",
    "        Flatten()(Embedding(dim_organisms, 20, input_length=len_organisms)(bonus_info[2]))\n",
    "    ]\n",
    "        \n",
    "    \n",
    "    in_sentence = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int64')\n",
    "    embedded_sentence = e(in_sentence)\n",
    "    #conv_sentence = embedded_sentence\n",
    "    #conv_sentence =  Dropout(0.2)(embedded_sentence)\n",
    "    #conv_sentence =  Conv1D(256, 3, activation='relu')(conv_sentence)\n",
    "    #conv_sentence = GlobalMaxPooling1D()(conv_sentence)\n",
    "    #encoded_model = keras.Model(in_sentence, conv_sentence)\n",
    "\n",
    "\n",
    "    lstm_sentence = LSTM(64)(embedded_sentence)\n",
    "    encoded_model = keras.Model(in_sentence, lstm_sentence)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sentences, MAX_SEQUENCE_LENGTH), dtype='int64')\n",
    "    seq_encoded = TimeDistributed(encoded_model)(sequence_input)\n",
    "    seq_encoded = Dropout(0.2)(seq_encoded)\n",
    "\n",
    "    # Encode entire sentence\n",
    "    seq_encoded =  Dropout(0.2)(LSTM(64)(seq_encoded))\n",
    "    concat = Dropout(0.5)(keras.layers.concatenate([seq_encoded] + embedded_bonus_info))\n",
    "\n",
    "    # Prediction\n",
    "    out_layer = Dense(1, activation='sigmoid')(concat)\n",
    "    model = keras.Model([sequence_input] + bonus_info, out_layer)\n",
    "    r = stratified_experiment(model, [texts, meshTags, genes, organisms], labels, smart_fine_tune=tuner, max_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
